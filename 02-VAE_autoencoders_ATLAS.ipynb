{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = x_train.shape\n",
    "original_dim = image_size[1] * image_size[2]\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "input_shape = (batch_size, original_dim)\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr \n",
    "    an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 2 and 128 for 'z_6/mul_1' (op: 'Mul') with input shapes: [?,128,2], [?,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5d85f4481be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use reparameterization trick to push the sampling out as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# note that \"output_shape\" isn't necessary with the TensorFlow backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-fec43d37efba>\u001b[0m in \u001b[0;36msampling\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# by default, random_normal has mean=0 and std=1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    977\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   4757\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4758\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4759\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   4760\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4761\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 2 and 128 for 'z_6/mul_1' (op: 'Mul') with input shapes: [?,128,2], [?,128]."
     ]
    }
   ],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 1680000)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          860160512   encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 860,162,564\n",
      "Trainable params: 860,162,564\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1680000)           861840000 \n",
      "=================================================================\n",
      "Total params: 861,841,536\n",
      "Trainable params: 861,841,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_sign\"):\n",
    "    \"\"\"Plots labels and LC ATLAS signatures as function \n",
    "    of 2-dim latent vector\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"signs_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    sign_size = (18, 12)\n",
    "    figure = np.zeros((digit_size[0] * n, digit_size[1] * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(sign_size[0], sign_size[1])\n",
    "            figure[i * sign_size[0]: (i + 1) * sign_size[0],\n",
    "                   j * sign_size[1]: (j + 1) * sign_size[1]] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = sign_size[0] // 2\n",
    "    end_range = n * sign_size[0] + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, sign_size[0])\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected encoder_input to have shape (1680000,) but got array with shape (784,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-60fd5f2b9c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected encoder_input to have shape (1680000,) but got array with shape (784,)"
     ]
    }
   ],
   "source": [
    "vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test) #, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_encoded[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[0][:, 0], x_test_encoded[0][:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "image_size = (18, 12)\n",
    "original_dim = image_size[0] * image_size[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 32\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0.)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "h_decoded = decoder_h(z)\n",
    "\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# end-to-end autoencoder\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "#inputs = Input(shape=(18, 12), name='encoder_input') \n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "h = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(h)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(h)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# instantiate encoder model\n",
    "encoder = Model(inputs, z_mean, name='encoder')#[z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mira = pd.read_csv('ATLAS_LC/MIRA_features_table.csv')\n",
    "signature_cols = [col for col in df_mira.columns if 'Signature' in col]\n",
    "signature_cols += ['OBJID', 'filter', 'CLASS']\n",
    "df_mira = df_mira[signature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpulse = pd.read_csv('ATLAS_LC/MPULSE_features_table.csv')[signature_cols]\n",
    "df_dbf = pd.read_csv('ATLAS_LC/DBF_features_table.csv')[signature_cols]\n",
    "df_lpv = pd.read_csv('ATLAS_LC/LPV_features_table.csv')[signature_cols]\n",
    "df_dbh = pd.read_csv('ATLAS_LC/DBH_features_table.csv')[signature_cols]\n",
    "df_pulse = pd.read_csv('ATLAS_LC/PULSE_features_table.csv')[signature_cols]\n",
    "df_nsine = pd.read_csv('ATLAS_LC/NSINE_features_table.csv')[signature_cols]\n",
    "df_sine = pd.read_csv('ATLAS_LC/SINE_features_table.csv')[signature_cols]\n",
    "df_msine = pd.read_csv('ATLAS_LC/MSINE_features_table.csv')[signature_cols]\n",
    "df_cbh = pd.read_csv('ATLAS_LC/CBH_features_table.csv')[signature_cols]\n",
    "df_cbf = pd.read_csv('ATLAS_LC/CBF_features_table.csv')[signature_cols]\n",
    "df_irr = pd.read_csv('ATLAS_LC/IRR_features_table.csv')[signature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Signature_ph_00_mag_00', u'Signature_ph_00_mag_01',\n",
       "       u'Signature_ph_00_mag_02', u'Signature_ph_00_mag_03',\n",
       "       u'Signature_ph_00_mag_04', u'Signature_ph_00_mag_05',\n",
       "       u'Signature_ph_00_mag_06', u'Signature_ph_00_mag_07',\n",
       "       u'Signature_ph_00_mag_08', u'Signature_ph_00_mag_09',\n",
       "       ...\n",
       "       u'Signature_ph_17_mag_05', u'Signature_ph_17_mag_06',\n",
       "       u'Signature_ph_17_mag_07', u'Signature_ph_17_mag_08',\n",
       "       u'Signature_ph_17_mag_09', u'Signature_ph_17_mag_10',\n",
       "       u'Signature_ph_17_mag_11', u'OBJID', u'filter', u'CLASS'],\n",
       "      dtype='object', length=219)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mira.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([df_mira, df_mpulse, df_dbf, df_lpv, df_dbh, df_pulse, \n",
    "                       df_nsine, df_sine, df_msine, df_cbf, df_cbh], sort=False)\n",
    "signature_cols = [col for col in df_mira.columns if 'Signature' in col]\n",
    "X = full_data[signature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_data['CLASS'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_mira)\n",
    "del(df_dbf)\n",
    "del(df_lpv)\n",
    "del(df_dbh)\n",
    "del(df_pulse)\n",
    "del(df_nsine)\n",
    "del(df_sine)\n",
    "del(df_msine)\n",
    "del(df_cbh)\n",
    "del(df_cbf)\n",
    "del(df_irr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.reshape(X.shape[0], 18, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "signature_cols = [col for col in df_mira.columns if 'Signature' in col]\n",
    "data = df_mira[signature_cols].values\n",
    "data = data.reshape(data.shape[0], 18, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(data, test_size=0.50, random_state=21)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181266, 216)\n",
      "(181266, 216)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181266 samples, validate on 181266 samples\n",
      "Epoch 1/300\n",
      "180352/181266 [============================>.] - ETA: 0s - loss: 0.0113"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [128,216] vs. [18,216]\n\t [[Node: training/RMSprop/gradients/loss/dense_8_loss/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@train...ad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/RMSprop/gradients/loss/dense_8_loss/logistic_loss/mul_grad/Shape, training/RMSprop/gradients/loss/dense_8_loss/logistic_loss/mul_grad/Shape_1)]]\n\t [[Node: loss/mul/_67 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_545_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-486e069a5086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 validation_data=(x_test, x_test))#,\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0;31m#callbacks=[TensorBoard(log_dir='/users/bsanchez/tensorboard_logs/autoencoder')])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/bsanchez/.virtualenvs/benv/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [128,216] vs. [18,216]\n\t [[Node: training/RMSprop/gradients/loss/dense_8_loss/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@train...ad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/RMSprop/gradients/loss/dense_8_loss/logistic_loss/mul_grad/Shape, training/RMSprop/gradients/loss/dense_8_loss/logistic_loss/mul_grad/Shape_1)]]\n\t [[Node: loss/mul/_67 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_545_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "#with tf.device(':/gpu:2'):\n",
    "vae.fit(x_test, x_test,\n",
    "                epochs=300,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))#,\n",
    "                #callbacks=[TensorBoard(log_dir='/users/bsanchez/tensorboard_logs/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "# encode and decode some lightcurves\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs_full = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 6  # how many digits we will display\n",
    "plt.figure(figsize=(24, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    j = np.random.randint(0, high=len(X_test))\n",
    "    plt.imshow(X_test[j, :, :, 0].T, cmap='inferno')\n",
    "    \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs_full[j, :, :, 0].T, cmap='inferno')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display encoded view\n",
    "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "    plt.imshow(encoded_imgs[j].reshape(12, 6).T, cmap='inferno')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362532, 3, 3, 8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode every data \n",
    "encoded_imgs = encoder.predict(data)\n",
    "encoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362532, 18, 12, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "decoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('deep_encoded_signatures.npy', encoded_imgs)\n",
    "np.save('deep_decoded_signatures.npy', decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
