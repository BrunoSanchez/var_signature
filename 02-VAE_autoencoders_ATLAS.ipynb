{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr \n",
    "    an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_sign\"):\n",
    "    \"\"\"Plots labels and LC ATLAS signatures as function \n",
    "    of 2-dim latent vector\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"signs_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    sign_size = (18, 12)\n",
    "    figure = np.zeros((digit_size[0] * n, digit_size[1] * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(sign_size[0], sign_size[1])\n",
    "            figure[i * sign_size[0]: (i + 1) * sign_size[0],\n",
    "                   j * sign_size[1]: (j + 1) * sign_size[1]] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = sign_size[0] // 2\n",
    "    end_range = n * sign_size[0] + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, sign_size[0])\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (18, 12)\n",
    "original_dim = image_size[0] * image_size[1]\n",
    "#x_train = np.reshape(x_train, [-1, original_dim])\n",
    "#x_test = np.reshape(x_test, [-1, original_dim])\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 216)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          111104      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 113,156\n",
      "Trainable params: 113,156\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 216)               110808    \n",
      "=================================================================\n",
      "Total params: 112,344\n",
      "Trainable params: 112,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 216)               0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 113156    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 216)               112344    \n",
      "=================================================================\n",
      "Total params: 225,500\n",
      "Trainable params: 225,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 18, 12, 16)\n",
      "(?, 9, 6, 16)\n",
      "(?, 9, 6, 8)\n",
      "(?, 3, 3, 8)\n",
      "This is the bottleneck\n",
      "(?, 3, 3, 8)\n",
      "(?, 9, 6, 8)\n",
      "(?, 9, 6, 16)\n",
      "(?, 18, 12, 16)\n",
      "(?, 18, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(18, 12, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "print x.shape\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "print x.shape\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "print x.shape\n",
    "#x = MaxPooling2D((3, 2), padding='same')(x)\n",
    "#print x.shape\n",
    "#x = Conv2D(8, (3, 2), activation='relu', padding='same')(x)\n",
    "#print x.shape\n",
    "encoded = MaxPooling2D((3, 2), padding='same')(x)\n",
    "print encoded.shape\n",
    "\n",
    "# at this point the representation is ### altered(3, 3, 8) i.e. 128-dimensional\n",
    "print 'This is the bottleneck'\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "print x.shape\n",
    "x = UpSampling2D((3, 2))(x)\n",
    "print x.shape\n",
    "#x = Conv2D(8, (2, 2), activation='relu', padding='same')(x)\n",
    "#x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "print x.shape\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "print x.shape\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "print decoded.shape\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(3, 3, 8))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layers = autoencoder.layers[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composed_model = decoder_layers[0](encoded_input)\n",
    "composed_model = decoder_layers[1](composed_model)\n",
    "composed_model = decoder_layers[2](composed_model)\n",
    "composed_model = decoder_layers[3](composed_model)\n",
    "composed_model = decoder_layers[4](composed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the decoder model\n",
    "decoder = Model(encoded_input, composed_model)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mira = pd.read_csv('ATLAS_LC/MIRA_features_table.csv')\n",
    "signature_cols = [col for col in df_mira.columns if 'Signature' in col]\n",
    "signature_cols += ['OBJID', 'filter', 'CLASS']\n",
    "df_mira = df_mira[signature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mpulse = pd.read_csv('ATLAS_LC/MPULSE_features_table.csv')[signature_cols]\n",
    "df_dbf = pd.read_csv('ATLAS_LC/DBF_features_table.csv')[signature_cols]\n",
    "df_lpv = pd.read_csv('ATLAS_LC/LPV_features_table.csv')[signature_cols]\n",
    "df_dbh = pd.read_csv('ATLAS_LC/DBH_features_table.csv')[signature_cols]\n",
    "df_pulse = pd.read_csv('ATLAS_LC/PULSE_features_table.csv')[signature_cols]\n",
    "df_nsine = pd.read_csv('ATLAS_LC/NSINE_features_table.csv')[signature_cols]\n",
    "df_sine = pd.read_csv('ATLAS_LC/SINE_features_table.csv')[signature_cols]\n",
    "df_msine = pd.read_csv('ATLAS_LC/MSINE_features_table.csv')[signature_cols]\n",
    "df_cbh = pd.read_csv('ATLAS_LC/CBH_features_table.csv')[signature_cols]\n",
    "df_cbf = pd.read_csv('ATLAS_LC/CBF_features_table.csv')[signature_cols]\n",
    "df_irr = pd.read_csv('ATLAS_LC/IRR_features_table.csv')[signature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data = pd.concat([df_mira, df_mpulse, df_dbf, df_lpv, df_dbh, df_pulse, \n",
    "                       df_nsine, df_sine, df_msine, df_cbf, df_cbh], sort=False)\n",
    "signature_cols = [col for col in df_mira.columns if 'Signature' in col]\n",
    "X = full_data[signature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(df_mira)\n",
    "del(df_dbf)\n",
    "del(df_lpv)\n",
    "del(df_dbh)\n",
    "del(df_pulse)\n",
    "del(df_nsine)\n",
    "del(df_sine)\n",
    "del(df_msine)\n",
    "del(df_cbh)\n",
    "del(df_cbf)\n",
    "del(df_irr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = X.reshape(X.shape[0], 18, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "signature_cols = [col for col in df_mira.columns if 'Signature' in col]\n",
    "data = df_mira[signature_cols].values\n",
    "data = data.reshape(data.shape[0], 18, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.10, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326278, 18, 12, 1)\n",
      "(36254, 18, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 326278 samples, validate on 36254 samples\n",
      "Epoch 1/300\n",
      "326278/326278 [==============================] - 4s 13us/step - loss: -0.5760 - val_loss: -1.0642\n",
      "Epoch 2/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.1561 - val_loss: -1.2555\n",
      "Epoch 3/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.2855 - val_loss: -1.3310\n",
      "Epoch 4/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.3528 - val_loss: -1.3923\n",
      "Epoch 5/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.3960 - val_loss: -1.4254\n",
      "Epoch 6/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.4267 - val_loss: -1.4458\n",
      "Epoch 7/300\n",
      "326278/326278 [==============================] - 3s 11us/step - loss: -1.4510 - val_loss: -1.4811\n",
      "Epoch 8/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.4687 - val_loss: -1.4702\n",
      "Epoch 9/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.4846 - val_loss: -1.4849\n",
      "Epoch 10/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.4992 - val_loss: -1.4901\n",
      "Epoch 11/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5106 - val_loss: -1.5299\n",
      "Epoch 12/300\n",
      "326278/326278 [==============================] - 3s 11us/step - loss: -1.5216 - val_loss: -1.5337\n",
      "Epoch 13/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.5303 - val_loss: -1.5484\n",
      "Epoch 14/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5384 - val_loss: -1.5578\n",
      "Epoch 15/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.5459 - val_loss: -1.5601\n",
      "Epoch 16/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5521 - val_loss: -1.5467\n",
      "Epoch 17/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5586 - val_loss: -1.5649\n",
      "Epoch 18/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5636 - val_loss: -1.5644\n",
      "Epoch 19/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5675 - val_loss: -1.5682\n",
      "Epoch 20/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5724 - val_loss: -1.5883\n",
      "Epoch 21/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5792 - val_loss: -1.5894\n",
      "Epoch 22/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5847 - val_loss: -1.6021\n",
      "Epoch 23/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.5904 - val_loss: -1.5828\n",
      "Epoch 24/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.5944 - val_loss: -1.6157\n",
      "Epoch 25/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.5984 - val_loss: -1.6030\n",
      "Epoch 26/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6015 - val_loss: -1.6035\n",
      "Epoch 27/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6048 - val_loss: -1.6155\n",
      "Epoch 28/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6079 - val_loss: -1.6301\n",
      "Epoch 29/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6126 - val_loss: -1.5934\n",
      "Epoch 30/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6175 - val_loss: -1.6159\n",
      "Epoch 31/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6218 - val_loss: -1.6131\n",
      "Epoch 32/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6265 - val_loss: -1.6232\n",
      "Epoch 33/300\n",
      "326278/326278 [==============================] - 3s 11us/step - loss: -1.6306 - val_loss: -1.6406\n",
      "Epoch 34/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6336 - val_loss: -1.6490\n",
      "Epoch 35/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6354 - val_loss: -1.6457\n",
      "Epoch 36/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6387 - val_loss: -1.6512\n",
      "Epoch 37/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6430 - val_loss: -1.6623\n",
      "Epoch 38/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6444 - val_loss: -1.6533\n",
      "Epoch 39/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6470 - val_loss: -1.6675\n",
      "Epoch 40/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6499 - val_loss: -1.6557\n",
      "Epoch 41/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6504 - val_loss: -1.6578\n",
      "Epoch 42/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6522 - val_loss: -1.6674\n",
      "Epoch 43/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6536 - val_loss: -1.6657\n",
      "Epoch 44/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6554 - val_loss: -1.6698\n",
      "Epoch 45/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6566 - val_loss: -1.6742\n",
      "Epoch 46/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6601 - val_loss: -1.6673\n",
      "Epoch 47/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6637 - val_loss: -1.6721\n",
      "Epoch 48/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6651 - val_loss: -1.6643\n",
      "Epoch 49/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6666 - val_loss: -1.6813\n",
      "Epoch 50/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6669 - val_loss: -1.6716\n",
      "Epoch 51/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6671 - val_loss: -1.6640\n",
      "Epoch 52/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6694 - val_loss: -1.6892\n",
      "Epoch 53/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6706 - val_loss: -1.6776\n",
      "Epoch 54/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6714 - val_loss: -1.6559\n",
      "Epoch 55/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6726 - val_loss: -1.6870\n",
      "Epoch 56/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6733 - val_loss: -1.6601\n",
      "Epoch 57/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6736 - val_loss: -1.6752\n",
      "Epoch 58/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6767 - val_loss: -1.6799\n",
      "Epoch 59/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6780 - val_loss: -1.6975\n",
      "Epoch 60/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6789 - val_loss: -1.6831\n",
      "Epoch 61/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6797 - val_loss: -1.6961\n",
      "Epoch 62/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6807 - val_loss: -1.6919\n",
      "Epoch 63/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6817 - val_loss: -1.7000\n",
      "Epoch 64/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6826 - val_loss: -1.6978\n",
      "Epoch 65/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6853 - val_loss: -1.6744\n",
      "Epoch 66/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6861 - val_loss: -1.6735\n",
      "Epoch 67/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6889 - val_loss: -1.6795\n",
      "Epoch 68/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6897 - val_loss: -1.6711\n",
      "Epoch 69/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6928 - val_loss: -1.6902\n",
      "Epoch 70/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6939 - val_loss: -1.7060\n",
      "Epoch 71/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6944 - val_loss: -1.7042\n",
      "Epoch 72/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6947 - val_loss: -1.7080\n",
      "Epoch 73/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6961 - val_loss: -1.7142\n",
      "Epoch 74/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6959 - val_loss: -1.7133\n",
      "Epoch 75/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7005 - val_loss: -1.7178\n",
      "Epoch 76/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6985 - val_loss: -1.7033\n",
      "Epoch 77/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6988 - val_loss: -1.7066\n",
      "Epoch 78/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.6983 - val_loss: -1.7147\n",
      "Epoch 79/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7002 - val_loss: -1.7063\n",
      "Epoch 80/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.6995 - val_loss: -1.6999\n",
      "Epoch 81/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7011 - val_loss: -1.7110\n",
      "Epoch 82/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7013 - val_loss: -1.7062\n",
      "Epoch 83/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7029 - val_loss: -1.7104\n",
      "Epoch 84/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7028 - val_loss: -1.6912\n",
      "Epoch 85/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7053 - val_loss: -1.7242\n",
      "Epoch 86/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7068 - val_loss: -1.7193\n",
      "Epoch 87/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7078 - val_loss: -1.6992\n",
      "Epoch 88/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7081 - val_loss: -1.7214\n",
      "Epoch 89/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7100 - val_loss: -1.7279\n",
      "Epoch 90/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7103 - val_loss: -1.7285\n",
      "Epoch 91/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7117 - val_loss: -1.7259\n",
      "Epoch 92/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7147 - val_loss: -1.7242\n",
      "Epoch 93/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7146 - val_loss: -1.7251\n",
      "Epoch 94/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7152 - val_loss: -1.7310\n",
      "Epoch 95/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7141 - val_loss: -1.7216\n",
      "Epoch 96/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7165 - val_loss: -1.7356\n",
      "Epoch 97/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7157 - val_loss: -1.7222\n",
      "Epoch 98/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7149 - val_loss: -1.7111\n",
      "Epoch 99/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7198 - val_loss: -1.7280\n",
      "Epoch 100/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7189 - val_loss: -1.7187\n",
      "Epoch 101/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7187 - val_loss: -1.7166\n",
      "Epoch 102/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7196 - val_loss: -1.7191\n",
      "Epoch 103/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7203 - val_loss: -1.7292\n",
      "Epoch 104/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7209 - val_loss: -1.7213\n",
      "Epoch 105/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7218 - val_loss: -1.7200\n",
      "Epoch 106/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7226 - val_loss: -1.7316\n",
      "Epoch 107/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7210 - val_loss: -1.7307\n",
      "Epoch 108/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7220 - val_loss: -1.7276\n",
      "Epoch 109/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7207 - val_loss: -1.7181\n",
      "Epoch 110/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7204 - val_loss: -1.7369\n",
      "Epoch 111/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7211 - val_loss: -1.7425\n",
      "Epoch 112/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7212 - val_loss: -1.7330\n",
      "Epoch 113/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7238 - val_loss: -1.7363\n",
      "Epoch 114/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7245 - val_loss: -1.7296\n",
      "Epoch 115/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7252 - val_loss: -1.7376\n",
      "Epoch 116/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7243 - val_loss: -1.7418\n",
      "Epoch 117/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7253 - val_loss: -1.7440\n",
      "Epoch 118/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7244 - val_loss: -1.7391\n",
      "Epoch 119/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7265 - val_loss: -1.7449\n",
      "Epoch 120/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7269 - val_loss: -1.7296\n",
      "Epoch 121/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7262 - val_loss: -1.7426\n",
      "Epoch 122/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7266 - val_loss: -1.7267\n",
      "Epoch 123/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7273 - val_loss: -1.7389\n",
      "Epoch 124/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7271 - val_loss: -1.7504\n",
      "Epoch 125/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7288 - val_loss: -1.7312\n",
      "Epoch 126/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7308 - val_loss: -1.7568\n",
      "Epoch 127/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7298 - val_loss: -1.7486\n",
      "Epoch 128/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7320 - val_loss: -1.7405\n",
      "Epoch 129/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7305 - val_loss: -1.7091\n",
      "Epoch 130/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7290 - val_loss: -1.7502\n",
      "Epoch 131/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7294 - val_loss: -1.7226\n",
      "Epoch 132/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7313 - val_loss: -1.7329\n",
      "Epoch 133/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7285 - val_loss: -1.7509\n",
      "Epoch 134/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7303 - val_loss: -1.7457\n",
      "Epoch 135/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7319 - val_loss: -1.7429\n",
      "Epoch 136/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7310 - val_loss: -1.7353\n",
      "Epoch 137/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7325 - val_loss: -1.7449\n",
      "Epoch 138/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7360 - val_loss: -1.7510\n",
      "Epoch 139/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7352 - val_loss: -1.7468\n",
      "Epoch 140/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7347 - val_loss: -1.7438\n",
      "Epoch 141/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7359 - val_loss: -1.7441\n",
      "Epoch 142/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7326 - val_loss: -1.7508\n",
      "Epoch 143/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7333 - val_loss: -1.7393\n",
      "Epoch 144/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7326 - val_loss: -1.7311\n",
      "Epoch 145/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7327 - val_loss: -1.7519\n",
      "Epoch 146/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7331 - val_loss: -1.7257\n",
      "Epoch 147/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7327 - val_loss: -1.7338\n",
      "Epoch 148/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7371 - val_loss: -1.7185\n",
      "Epoch 149/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7332 - val_loss: -1.7254\n",
      "Epoch 150/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7343 - val_loss: -1.7422\n",
      "Epoch 151/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7349 - val_loss: -1.7331\n",
      "Epoch 152/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7318 - val_loss: -1.7456\n",
      "Epoch 153/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7338 - val_loss: -1.7491\n",
      "Epoch 154/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7338 - val_loss: -1.7529\n",
      "Epoch 155/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7344 - val_loss: -1.7264\n",
      "Epoch 156/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7357 - val_loss: -1.7286\n",
      "Epoch 157/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7342 - val_loss: -1.7396\n",
      "Epoch 158/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7351 - val_loss: -1.7469\n",
      "Epoch 159/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7361 - val_loss: -1.7528\n",
      "Epoch 160/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7371 - val_loss: -1.7432\n",
      "Epoch 161/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7368 - val_loss: -1.7275\n",
      "Epoch 162/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7381 - val_loss: -1.7380\n",
      "Epoch 163/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7369 - val_loss: -1.7499\n",
      "Epoch 164/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7367 - val_loss: -1.7544\n",
      "Epoch 165/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7385 - val_loss: -1.7345\n",
      "Epoch 166/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7376 - val_loss: -1.7499\n",
      "Epoch 167/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7365 - val_loss: -1.7502\n",
      "Epoch 168/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7361 - val_loss: -1.7547\n",
      "Epoch 169/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7367 - val_loss: -1.7284\n",
      "Epoch 170/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7371 - val_loss: -1.7336\n",
      "Epoch 171/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7401 - val_loss: -1.7578\n",
      "Epoch 172/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7403 - val_loss: -1.7213\n",
      "Epoch 173/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7388 - val_loss: -1.7430\n",
      "Epoch 174/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7385 - val_loss: -1.7598\n",
      "Epoch 175/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7390 - val_loss: -1.7603\n",
      "Epoch 176/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7404 - val_loss: -1.7590\n",
      "Epoch 177/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7433 - val_loss: -1.7463\n",
      "Epoch 178/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7419 - val_loss: -1.7450\n",
      "Epoch 179/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7448 - val_loss: -1.7604\n",
      "Epoch 180/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7419 - val_loss: -1.7543\n",
      "Epoch 181/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7430 - val_loss: -1.7435\n",
      "Epoch 182/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7406 - val_loss: -1.7537\n",
      "Epoch 183/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7422 - val_loss: -1.7317\n",
      "Epoch 184/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7445 - val_loss: -1.7657\n",
      "Epoch 185/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7440 - val_loss: -1.7349\n",
      "Epoch 186/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7415 - val_loss: -1.7657\n",
      "Epoch 187/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7420 - val_loss: -1.7177\n",
      "Epoch 188/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7409 - val_loss: -1.7333\n",
      "Epoch 189/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7412 - val_loss: -1.7595\n",
      "Epoch 190/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7480 - val_loss: -1.7610\n",
      "Epoch 191/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7454 - val_loss: -1.7592\n",
      "Epoch 192/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7439 - val_loss: -1.7383\n",
      "Epoch 193/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7438 - val_loss: -1.7583\n",
      "Epoch 194/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7469 - val_loss: -1.7514\n",
      "Epoch 195/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7461 - val_loss: -1.7606\n",
      "Epoch 196/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7455 - val_loss: -1.7219\n",
      "Epoch 197/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7463 - val_loss: -1.7334\n",
      "Epoch 198/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7451 - val_loss: -1.7650\n",
      "Epoch 199/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7451 - val_loss: -1.7525\n",
      "Epoch 200/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7460 - val_loss: -1.7466\n",
      "Epoch 201/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7441 - val_loss: -1.7490\n",
      "Epoch 202/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7484 - val_loss: -1.7626\n",
      "Epoch 203/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7456 - val_loss: -1.7649\n",
      "Epoch 204/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7480 - val_loss: -1.7398\n",
      "Epoch 205/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7464 - val_loss: -1.7429\n",
      "Epoch 206/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7503 - val_loss: -1.7650\n",
      "Epoch 207/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7475 - val_loss: -1.7631\n",
      "Epoch 208/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7498 - val_loss: -1.7610\n",
      "Epoch 209/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7480 - val_loss: -1.7396\n",
      "Epoch 210/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7477 - val_loss: -1.7629\n",
      "Epoch 211/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7495 - val_loss: -1.7500\n",
      "Epoch 212/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7475 - val_loss: -1.7398\n",
      "Epoch 213/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7474 - val_loss: -1.7444\n",
      "Epoch 214/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7481 - val_loss: -1.7515\n",
      "Epoch 215/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7473 - val_loss: -1.7592\n",
      "Epoch 216/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7479 - val_loss: -1.7574\n",
      "Epoch 217/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7476 - val_loss: -1.7537\n",
      "Epoch 218/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7467 - val_loss: -1.7497\n",
      "Epoch 219/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7494 - val_loss: -1.7582\n",
      "Epoch 220/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7482 - val_loss: -1.7603\n",
      "Epoch 221/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7509 - val_loss: -1.7585\n",
      "Epoch 222/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7492 - val_loss: -1.7519\n",
      "Epoch 223/300\n",
      "326278/326278 [==============================] - 3s 11us/step - loss: -1.7496 - val_loss: -1.7601\n",
      "Epoch 224/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7511 - val_loss: -1.7556\n",
      "Epoch 225/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7483 - val_loss: -1.7522\n",
      "Epoch 226/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7504 - val_loss: -1.7521\n",
      "Epoch 227/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7516 - val_loss: -1.7505\n",
      "Epoch 228/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7514 - val_loss: -1.7526\n",
      "Epoch 229/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7522 - val_loss: -1.7426\n",
      "Epoch 230/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7488 - val_loss: -1.7573\n",
      "Epoch 231/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7484 - val_loss: -1.7532\n",
      "Epoch 232/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7506 - val_loss: -1.7519\n",
      "Epoch 233/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7495 - val_loss: -1.7510\n",
      "Epoch 234/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7494 - val_loss: -1.7398\n",
      "Epoch 235/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7502 - val_loss: -1.7659\n",
      "Epoch 236/300\n",
      "326278/326278 [==============================] - 4s 12us/step - loss: -1.7515 - val_loss: -1.7666\n",
      "Epoch 237/300\n",
      "326278/326278 [==============================] - 4s 11us/step - loss: -1.7507 - val_loss: -1.7401\n",
      "Epoch 238/300\n",
      "215000/326278 [==================>...........] - ETA: 1s - loss: -1.7506"
     ]
    }
   ],
   "source": [
    "#with tf.device(':/gpu:2'):\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=300,\n",
    "                batch_size=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[TensorBoard(log_dir='/users/bsanchez/tensorboard_logs/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "# encode and decode some lightcurves\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(X_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_imgs_full = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 6  # how many digits we will display\n",
    "plt.figure(figsize=(24, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    j = np.random.randint(0, high=len(X_test))\n",
    "    plt.imshow(X_test[j, :, :, 0].T, cmap='inferno')\n",
    "    \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs_full[j, :, :, 0].T, cmap='inferno')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display encoded view\n",
    "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "    plt.imshow(encoded_imgs[j].reshape(12, 6).T, cmap='inferno')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362532, 3, 3, 8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode every data \n",
    "encoded_imgs = encoder.predict(data)\n",
    "encoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362532, 18, 12, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "decoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('deep_encoded_signatures.npy', encoded_imgs)\n",
    "np.save('deep_decoded_signatures.npy', decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
